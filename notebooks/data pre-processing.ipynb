{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985b8b59",
   "metadata": {},
   "source": [
    "1. MODULE IMPORTATION: Loading essential libraries for synthetic data generation and temporal manipulation. This environment supports the creation of realistic user event logs and relational structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1039b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1276bf",
   "metadata": {},
   "source": [
    "2. DATABASE INTEGRATION: Initializing the SQLAlchemy engine to establish a persistent connection with the local PostgreSQL instance. This enables seamless migration from Python memory to relational storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9ec3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_url = \"postgresql://postgres:J.e.s.u.s01*@localhost:5432/projects\"\n",
    "engine = create_engine(database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75179251",
   "metadata": {},
   "source": [
    "3. DATA ARCHITECTURE: Defining core schemas and generating synthetic user records. Using NumPy's randomization tools, we simulate a realistic business environment with varied acquisition dates and user types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5876ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table creation and Data Insertion (Artificial Data using NumPy)\n",
    "\n",
    "# Initial setup for controlled random data generation\n",
    "np.random.seed(42) # Ensures reproducibility; the same data is generated every time\n",
    "n_users = 600 # Total sample size of users\n",
    "start_date = datetime(2025, 7, 1) # Baseline date for the simulation start\n",
    "\n",
    "# 1. dim_users: Creating the user dimension table\n",
    "user_list = []\n",
    "channels = ['Organic', 'Paid Ad', 'Referral'] # Traffic acquisition sources\n",
    "\n",
    "for i in range(1, n_users + 1):\n",
    "    # Generates a random signup date within a 180-day window from the start date\n",
    "    signup_delay = np.random.randint(0, 180)\n",
    "    signup_date = start_date + timedelta(days = signup_delay)\n",
    "    \n",
    "    # Defining user attributes: ID, Date, Channel (with weighted probability), and Retention Flag\n",
    "    user_list.append({\n",
    "        'user_id': i,  \n",
    "        'signup_date': signup_date,\n",
    "        'channel': np.random.choice(channels, p=[0.4, 0.4, 0.2]), # 40% Organic, 40% Paid, 20% Referral\n",
    "        'is_retained_type': np.random.choice([True, False], p=[0.55, 0.45]) # Base probability of retention\n",
    "    })\n",
    "\n",
    "# Converting the list of dictionaries into a Pandas DataFrame\n",
    "df_users = pd.DataFrame(user_list)\n",
    "\n",
    "# 2. dim_features: Catalog of platform functionalities\n",
    "features_data = [\n",
    "    {'feature_id': 1, 'feature_name': 'login'},\n",
    "    {'feature_id': 2, 'feature_name': 'profile_completion'},\n",
    "    {'feature_id': 3, 'feature_name': 'social_invite'},\n",
    "    {'feature_id': 4, 'feature_name': 'dashboard_customization'}\n",
    "]\n",
    "df_features = pd.DataFrame(features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9721b91",
   "metadata": {},
   "source": [
    "4. BEHAVIORAL SIMULATION: Generating transactional event logs (Fact Table). This logic simulates the 'Aha! Moment' (Profile Completion) for retained users and sets different lifespan parameters to create a high-fidelity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d59c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. fact_events: Simulating user behavior logs\n",
    "event_list = []\n",
    "\n",
    "for _, user in df_users.iterrows():\n",
    "    uid = user['user_id']\n",
    "    s_date = user['signup_date']\n",
    "    is_retained = user['is_retained_type']\n",
    "    \n",
    "    # Every user logs an initial 'login' event upon registration\n",
    "    event_list.append({'user_id': uid, 'feature_id': 1, 'timestamp': s_date})\n",
    "    \n",
    "    # Business Logic: The \"Aha! Moment\" (profile completion) probability depends on retention status\n",
    "    prob_completion = 0.85 if is_retained else 0.15\n",
    "    \n",
    "    # Simulating profile completion based on the probability above\n",
    "    if np.random.random() < prob_completion:\n",
    "        # Event occurs between 1 and 48 hours after signup\n",
    "        comp_time = s_date + timedelta(hours=np.random.randint(1, 48))\n",
    "        event_list.append({'user_id': uid, 'feature_id': 2, 'timestamp': comp_time})\n",
    "        \n",
    "        # Defining how many days the user interacts before \"churning\"\n",
    "        lifespan = np.random.randint(30, 120) if is_retained else np.random.randint(2, 10)\n",
    "        \n",
    "        # Generating recurring events (login, social, or dashboard) throughout their lifespan\n",
    "        for d in range(1, lifespan, np.random.randint(1, 5)):\n",
    "            ts = s_date + timedelta(days=d, hours=np.random.randint(0, 23))\n",
    "            feat = np.random.choice([1, 3, 4])\n",
    "            event_list.append({'user_id': uid, 'feature_id': feat, 'timestamp': ts})\n",
    "    else:\n",
    "        # If the user DOES NOT complete the profile, they show minimal login activity for 2-3 days\n",
    "        for d in range(1, np.random.randint(2, 4)):\n",
    "            ts = s_date + timedelta(days=d)\n",
    "            event_list.append({'user_id': uid, 'feature_id': 1, 'timestamp': ts})\n",
    "\n",
    "# Consolidating events into a Fact Table DataFrame\n",
    "df_events = pd.DataFrame(event_list)\n",
    "\n",
    "# Data Cleaning: Removing the helper column used for simulation before final export\n",
    "df_users = df_users.drop(columns=['is_retained_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194aa4e",
   "metadata": {},
   "source": [
    "5. ETL PROCESS: Migrating the generated DataFrames into the PostgreSQL database. Utilizing a 'replace' strategy ensures a clean environment for each execution of the simulation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacc044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\"users\" : df_users, \"events\" : df_events, \"features\" : df_features}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    df.to_sql(name, engine, if_exists = \"replace\", index = False)\n",
    "    df.to_csv(f\"{name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
